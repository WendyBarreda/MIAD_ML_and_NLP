{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Construcción e implementación de modelos Bagging, Random Forest y XGBoost\n",
    "\n",
    "En este taller podrán poner en práctica sus conocimientos sobre la construcción e implementación de modelos de Bagging, Random Forest y XGBoost. El taller está constituido por 8 puntos, en los cuales deberan seguir las intrucciones de cada numeral para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos predicción precio de automóviles\n",
    "\n",
    "En este taller se usará el conjunto de datos de Car Listings de Kaggle donde cada observación representa el precio de un automóvil teniendo en cuenta distintas variables como año, marca, modelo, entre otras. El objetivo es predecir si el precio del automóvil es alto o no. Para más detalles puede visitar el siguiente enlace: [datos](https://www.kaggle.com/jpayne/852k-used-car-listings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>M_Camry</th>\n",
       "      <th>M_Camry4dr</th>\n",
       "      <th>M_CamryBase</th>\n",
       "      <th>M_CamryL</th>\n",
       "      <th>M_CamryLE</th>\n",
       "      <th>M_CamrySE</th>\n",
       "      <th>M_CamryXLE</th>\n",
       "      <th>HighPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014</td>\n",
       "      <td>6480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014</td>\n",
       "      <td>39972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2016</td>\n",
       "      <td>18989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2014</td>\n",
       "      <td>51330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2007</td>\n",
       "      <td>116065</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Mileage  M_Camry  M_Camry4dr  M_CamryBase  M_CamryL  M_CamryLE  \\\n",
       "7    2014     6480        0           0            0         1          0   \n",
       "11   2014    39972        0           0            0         0          1   \n",
       "167  2016    18989        0           0            0         0          0   \n",
       "225  2014    51330        0           0            0         1          0   \n",
       "270  2007   116065        0           1            0         0          0   \n",
       "\n",
       "     M_CamrySE  M_CamryXLE  HighPrice  \n",
       "7            0           0          1  \n",
       "11           0           0          0  \n",
       "167          1           0          1  \n",
       "225          0           0          0  \n",
       "270          0           0          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importación de librerías\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Lectura de la información de archivo .csv\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/dataTrain_carListings.zip')\n",
    "\n",
    "# Preprocesamiento de datos para el taller\n",
    "data = data.loc[data['Model'].str.contains('Camry')].drop(['Make', 'State'], axis=1)\n",
    "data = data.join(pd.get_dummies(data['Model'], prefix='M'))\n",
    "data['HighPrice'] = (data['Price'] > data['Price'].mean()).astype(int)\n",
    "data = data.drop(['Model', 'Price'], axis=1)\n",
    "\n",
    "# Visualización dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y)\n",
    "y = data['HighPrice']\n",
    "X = data.drop(['HighPrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en set de entrenamiento y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Árbol de decisión manual\n",
    "\n",
    "En la celda 1 creen un árbol de decisión **manualmente**  que considere los set de entrenamiento y test definidos anteriormente y presenten el acurracy del modelo en el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_pred': 1, 'y_prob': 0.5778472913408218, 'level': 0, 'split': [1, 51394.909090909096], 'n_samples': 7031, 'gain': 0.23630504821132137, 'sl': {'y_pred': 1, 'y_prob': 0.8375781948168007, 'level': 1, 'split': [0, 2015.0], 'n_samples': 4474, 'gain': 0.03713896892744761, 'sl': {'y_pred': 1, 'y_prob': 0.6404416839199448, 'level': 2, 'split': [0, 2013.0], 'n_samples': 1447, 'gain': 0.043426293133466354, 'sl': {'y_pred': 0, 'y_prob': 0.2780487804878049, 'level': 3, 'split': [0, 2012.0], 'n_samples': 203, 'gain': 0.05394731382546758, 'sl': {'y_pred': 0, 'y_prob': 0.09195402298850575, 'level': 4, 'split': [1, 35652.36363636363], 'n_samples': 85, 'gain': 0.01352946029496721, 'sl': {'y_pred': 0, 'y_prob': 0.24, 'level': 5, 'split': -1, 'n_samples': 23, 'gain': 0.08132745221592097}, 'sr': {'y_pred': 0, 'y_prob': 0.046875, 'level': 5, 'split': -1, 'n_samples': 62, 'gain': 0.0038712154997273124}}, 'sr': {'y_pred': 0, 'y_prob': 0.4166666666666667, 'level': 4, 'split': [1, 24887.363636363636], 'n_samples': 118, 'gain': 0.020017026523643233, 'sl': {'y_pred': 1, 'y_prob': 0.6923076923076923, 'level': 5, 'split': -1, 'n_samples': 11, 'gain': 0.11639118457300279}, 'sr': {'y_pred': 0, 'y_prob': 0.3853211009174312, 'level': 5, 'split': -1, 'n_samples': 107, 'gain': 0.02069617568904858}}}, 'sr': {'y_pred': 1, 'y_prob': 0.6998394863563403, 'level': 3, 'split': [1, 39822.0], 'n_samples': 1244, 'gain': 0.024256063237019176, 'sl': {'y_pred': 1, 'y_prob': 0.7671081677704195, 'level': 4, 'split': [6, 1.0], 'n_samples': 904, 'gain': 0.014011200973446158, 'sl': {'y_pred': 1, 'y_prob': 0.8110014104372355, 'level': 5, 'split': -1, 'n_samples': 707, 'gain': 0.00556156034847427}, 'sr': {'y_pred': 1, 'y_prob': 0.6080402010050251, 'level': 5, 'split': -1, 'n_samples': 197, 'gain': 0.0509520357534477}}, 'sr': {'y_pred': 1, 'y_prob': 0.52046783625731, 'level': 4, 'split': [6, 1.0], 'n_samples': 340, 'gain': 0.05453490718936188, 'sl': {'y_pred': 1, 'y_prob': 0.6098484848484849, 'level': 5, 'split': -1, 'n_samples': 262, 'gain': 0.016551370145274913}, 'sr': {'y_pred': 0, 'y_prob': 0.225, 'level': 5, 'split': -1, 'n_samples': 78, 'gain': 0.010898218590526254}}}}, 'sr': {'y_pred': 1, 'y_prob': 0.9316606140640475, 'level': 2, 'split': [1, 30532.545454545452], 'n_samples': 3027, 'gain': 0.007509453837285407, 'sl': {'y_pred': 1, 'y_prob': 0.9872958257713249, 'level': 3, 'split': -1, 'n_samples': 1651, 'gain': 0.00024133700317513843}, 'sr': {'y_pred': 1, 'y_prob': 0.8642960812772134, 'level': 3, 'split': [6, 1.0], 'n_samples': 1376, 'gain': 0.00816405585876312, 'sl': {'y_pred': 1, 'y_prob': 0.9044715447154471, 'level': 4, 'split': [1, 44211.63636363637], 'n_samples': 982, 'gain': 0.0011786261141377707, 'sl': {'y_pred': 1, 'y_prob': 0.9189944134078212, 'level': 5, 'split': -1, 'n_samples': 714, 'gain': 0.0015632224032787745}, 'sr': {'y_pred': 1, 'y_prob': 0.8629629629629629, 'level': 5, 'split': -1, 'n_samples': 268, 'gain': 0.0029666274073334997}}, 'sr': {'y_pred': 1, 'y_prob': 0.7626262626262627, 'level': 4, 'split': [1, 46088.36363636364], 'n_samples': 394, 'gain': 0.012432872082331625, 'sl': {'y_pred': 1, 'y_prob': 0.7993827160493827, 'level': 5, 'split': -1, 'n_samples': 322, 'gain': 0.0017790429580819}, 'sr': {'y_pred': 1, 'y_prob': 0.5945945945945946, 'level': 5, 'split': -1, 'n_samples': 72, 'gain': 0.0061262040428707865}}}}}, 'sr': {'y_pred': 0, 'y_prob': 0.12348573661586557, 'level': 1, 'split': [0, 2014.0], 'n_samples': 2557, 'gain': 0.047676779790769896, 'sl': {'y_pred': 0, 'y_prob': 0.03826530612244898, 'level': 2, 'split': [0, 2012.0], 'n_samples': 1958, 'gain': 0.0045681349880153654, 'sl': {'y_pred': 0, 'y_prob': 0.00727802037845706, 'level': 3, 'split': -1, 'n_samples': 1372, 'gain': 6.539810508043517e-05}, 'sr': {'y_pred': 0, 'y_prob': 0.11224489795918367, 'level': 3, 'split': [1, 73046.45454545454], 'n_samples': 586, 'gain': 0.011500104783552484, 'sl': {'y_pred': 0, 'y_prob': 0.21395348837209302, 'level': 4, 'split': [2, 0.7272727272727479], 'n_samples': 213, 'gain': 0.03131873045174638, 'sl': {'y_pred': 0, 'y_prob': 0.17435897435897435, 'level': 5, 'split': -1, 'n_samples': 193, 'gain': 0.0022441355404702046}, 'sr': {'y_pred': 1, 'y_prob': 0.5909090909090909, 'level': 5, 'split': -1, 'n_samples': 20, 'gain': 0.10666666666666669}}, 'sr': {'y_pred': 0, 'y_prob': 0.056, 'level': 4, 'split': [1, 86796.45454545454], 'n_samples': 373, 'gain': 0.0027469921069710745, 'sl': {'y_pred': 0, 'y_prob': 0.09883720930232558, 'level': 5, 'split': -1, 'n_samples': 170, 'gain': 0.005168695203297352}, 'sr': {'y_pred': 0, 'y_prob': 0.024390243902439025, 'level': 5, 'split': -1, 'n_samples': 203, 'gain': 0.0008738844612075081}}}}, 'sr': {'y_pred': 0, 'y_prob': 0.40266222961730447, 'level': 2, 'split': [0, 2015.0], 'n_samples': 599, 'gain': 0.03360777305969753, 'sl': {'y_pred': 0, 'y_prob': 0.30213903743315507, 'level': 3, 'split': [8, 1.0], 'n_samples': 372, 'gain': 0.02792228272448688, 'sl': {'y_pred': 0, 'y_prob': 0.2598187311178248, 'level': 4, 'split': [1, 60734.36363636364], 'n_samples': 329, 'gain': 0.020406917448241912, 'sl': {'y_pred': 0, 'y_prob': 0.39344262295081966, 'level': 5, 'split': -1, 'n_samples': 120, 'gain': 0.024785915984516682}, 'sr': {'y_pred': 0, 'y_prob': 0.1848341232227488, 'level': 5, 'split': -1, 'n_samples': 209, 'gain': 0.008907488615280956}}, 'sr': {'y_pred': 1, 'y_prob': 0.6222222222222222, 'level': 4, 'split': [1, 82538.7272727273], 'n_samples': 43, 'gain': 0.06528625511859693, 'sl': {'y_pred': 1, 'y_prob': 0.7027027027027027, 'level': 5, 'split': -1, 'n_samples': 35, 'gain': 0.027591836734693898}, 'sr': {'y_pred': 0, 'y_prob': 0.3, 'level': 5, 'split': -1, 'n_samples': 8, 'gain': 0.04166666666666674}}}, 'sr': {'y_pred': 1, 'y_prob': 0.5676855895196506, 'level': 3, 'split': [1, 81106.09090909094], 'n_samples': 227, 'gain': 0.02910175577937174, 'sl': {'y_pred': 1, 'y_prob': 0.6057692307692307, 'level': 4, 'split': [1, 56288.72727272727], 'n_samples': 206, 'gain': 0.01833397012522614, 'sl': {'y_pred': 1, 'y_prob': 0.7272727272727273, 'level': 5, 'split': -1, 'n_samples': 75, 'gain': 0.020388007054673807}, 'sr': {'y_pred': 1, 'y_prob': 0.5338345864661654, 'level': 5, 'split': -1, 'n_samples': 131, 'gain': 0.04889445575665913}}, 'sr': {'y_pred': 0, 'y_prob': 0.21739130434782608, 'level': 4, 'split': [8, 1.0], 'n_samples': 21, 'gain': 0.07558578987150422, 'sl': {'y_pred': 0, 'y_prob': 0.15, 'level': 5, 'split': -1, 'n_samples': 18, 'gain': 0.06419753086419752}, 'sr': {'y_pred': 1, 'y_prob': 0.6, 'level': 5, 'split': -1, 'n_samples': 3, 'gain': 0.4444444444444444}}}}}}\n",
      "-------------------Análisis de Accuracy-------------------\n",
      "verdaderos positivos: 1929\n",
      "verdaderos negativos: 1147\n",
      "falsos positivos: 291\n",
      "falsos negativos: 97\n",
      "0.8879907621247113\n"
     ]
    }
   ],
   "source": [
    "# Celda 1\n",
    "\n",
    "# Definición de la función que calcula el gini index\n",
    "def gini(y):\n",
    "    if y.shape[0] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - (y.mean()**2 + (1 - y.mean())**2)\n",
    "\n",
    "# Definición de la función gini_imputiry para calular la ganancia de una variable predictora j dado el punto de corte k    \n",
    "def gini_impurity(X_col, y, split):\n",
    "    \n",
    "    filter_l = X_col < split\n",
    "    y_l = y.loc[filter_l]\n",
    "    y_r = y.loc[~filter_l]\n",
    "    \n",
    "    n_l = y_l.shape[0]\n",
    "    n_r = y_r.shape[0]\n",
    "    \n",
    "    gini_y = gini(y)\n",
    "    gini_l = gini(y_l)\n",
    "    gini_r = gini(y_r)\n",
    "    \n",
    "    gini_impurity_ = gini_y - (n_l / (n_l + n_r) * gini_l + n_r / (n_l + n_r) * gini_r)\n",
    "    \n",
    "    return gini_impurity_\n",
    "\n",
    "\n",
    "# Definición de la función best_split para calcular cuál es la mejor variable y punto de cortepara hacer la bifurcación del árbol\n",
    "def best_split(X, y, num_pct=10):\n",
    "    \n",
    "    features = range(X.shape[1])\n",
    "    \n",
    "    best_split = [0, 0, 0]  # j, split, gain\n",
    "    \n",
    "    # Para todas las varibles \n",
    "    for j in features:\n",
    "        \n",
    "        splits = np.percentile(X.iloc[:, j], np.arange(0, 100, 100.0 / (num_pct+1)).tolist())\n",
    "        splits = np.unique(splits)[1:]\n",
    "        \n",
    "        # Para cada partición\n",
    "        for split in splits:\n",
    "            gain = gini_impurity(X.iloc[:, j], y, split)\n",
    "                        \n",
    "            if gain > best_split[2]:\n",
    "                best_split = [j, split, gain]\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "# Definición de la función tree_grow para hacer un crecimiento recursivo del árbol\n",
    "def tree_grow(X, y, level=0, min_gain=0.001, max_depth=None, num_pct=10):\n",
    "    \n",
    "    # Si solo es una observación\n",
    "    if X.shape[0] == 1:\n",
    "        tree = dict(y_pred=y.iloc[:1].values[0], y_prob=0.5, level=level, split=-1, n_samples=1, MSE=0)\n",
    "        return tree\n",
    "    \n",
    "    # Calcular la mejor división\n",
    "    j, split, gain = best_split(X, y, num_pct)\n",
    "    \n",
    "    # Guardar el árbol y estimar la predicción\n",
    "    y_pred = int(y.mean() >= 0.5) \n",
    "    y_prob = (y.sum() + 1.0) / (y.shape[0] + 2.0)  # Corrección Laplace \n",
    "    \n",
    "    tree = dict(y_pred=y_pred, y_prob=y_prob, level=level, split=-1, n_samples=X.shape[0], gain=gain)\n",
    "    # Revisar el criterio de parada \n",
    "    if gain < min_gain:\n",
    "        return tree\n",
    "    if max_depth is not None:\n",
    "        if level >= max_depth:\n",
    "            return tree   \n",
    "    \n",
    "    # Continuar creando la partición\n",
    "    filter_l = X.iloc[:, j] < split\n",
    "    X_l, y_l = X.loc[filter_l], y.loc[filter_l]\n",
    "    X_r, y_r = X.loc[~filter_l], y.loc[~filter_l]\n",
    "    tree['split'] = [j, split]\n",
    "\n",
    "    # Siguiente iteración para cada partición\n",
    "    \n",
    "    tree['sl'] = tree_grow(X_l, y_l, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    tree['sr'] = tree_grow(X_r, y_r, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# Aplicación de la función tree_grow\n",
    "tree=tree_grow(X_train, y_train, level=0, min_gain=0.001, max_depth=5, num_pct=10)\n",
    "print(tree)\n",
    "\n",
    "\n",
    "# Definición de la función tree_predict para hacer predicciones según las variables 'X' y el árbol 'tree'\n",
    "\n",
    "def tree_predict(X, tree, proba=False):\n",
    "    \n",
    "    predicted = np.ones(X.shape[0])\n",
    "\n",
    "    # Revisar si es el nodo final\n",
    "    if tree['split'] == -1:\n",
    "        if not proba:\n",
    "            predicted = predicted * tree['y_pred']\n",
    "        else:\n",
    "            predicted = predicted * tree['y_prob']\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        j, split = tree['split']\n",
    "        filter_l = (X.iloc[:, j] < split)\n",
    "        X_l = X.loc[filter_l]\n",
    "        X_r = X.loc[~filter_l]\n",
    "\n",
    "        if X_l.shape[0] == 0:  # Si el nodo izquierdo está vacio solo continua con el derecho \n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'], proba)\n",
    "        elif X_r.shape[0] == 0:  #  Si el nodo derecho está vacio solo continua con el izquierdo\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'], proba)\n",
    "        else:\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'], proba)\n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'], proba)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "predicciones=tree_predict(X_test, tree)\n",
    "\n",
    "\n",
    "predicciones1=predicciones.tolist()\n",
    "y_test1=y_test.tolist()\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "i=0\n",
    "listi=[]\n",
    "while i < len(predicciones1):\n",
    "    if predicciones1[i]==1:\n",
    "        if y_test1[i]==1:\n",
    "            tp=tp+1\n",
    "        else:\n",
    "            fp=fp+1\n",
    "    else:\n",
    "        if y_test1[i]==0:\n",
    "            tn=tn+1\n",
    "        else:\n",
    "            fn=fn+1\n",
    "    i=i+1\n",
    "    \n",
    "    \n",
    "print('-------------------Análisis de Accuracy-------------------')\n",
    "accuracy_tree=(tp+tn)/(tp+tn+fp+fn)\n",
    "print('verdaderos positivos: '+str(tp))\n",
    "print('verdaderos negativos: '+str(tn))\n",
    "print('falsos positivos: '+str(fp))\n",
    "print('falsos negativos: '+str(fn))\n",
    "print(accuracy_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2 - Bagging manual\n",
    "\n",
    "En la celda 2 creen un modelo bagging **manualmente** con 10 árboles de clasificación y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Árbol  0 tiene un accuracy:  0.8354503464203233\n",
      "Árbol  1 tiene un accuracy:  0.8342956120092379\n",
      "Árbol  2 tiene un accuracy:  0.8299653579676675\n",
      "Árbol  3 tiene un accuracy:  0.8386258660508084\n",
      "Árbol  4 tiene un accuracy:  0.8371824480369515\n",
      "Árbol  5 tiene un accuracy:  0.8409353348729792\n",
      "Árbol  6 tiene un accuracy:  0.8368937644341802\n",
      "Árbol  7 tiene un accuracy:  0.8400692840646651\n",
      "Árbol  8 tiene un accuracy:  0.8397806004618937\n",
      "Árbol  9 tiene un accuracy:  0.8296766743648961\n",
      "--------------------------------------\n",
      "Accuracy = 0.851905311778291\n"
     ]
    }
   ],
   "source": [
    "# Celda 2\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_estimators = 10 #Número de árboles y muestras bootstrap\n",
    "\n",
    "np.random.seed(123)\n",
    "n_samples = X_train.shape[0]\n",
    "samples = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(1, n_estimators +1 )]\n",
    "\n",
    "# Para que los arboles sean diferentes generamos semillas diferentes\n",
    "np.random.seed(123) \n",
    "seeds = np.random.randint(1, 10000, size=n_estimators)\n",
    "\n",
    "# DataFrame para guardar las predicciones de cada árbol\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=[list(range(n_estimators))])\n",
    "\n",
    "# Entrenamiento de 10 modelos con las 10 muestras boostrap\n",
    "for i, sample in enumerate(samples):\n",
    "    tree = DecisionTreeClassifier(max_features=\"sqrt\", max_depth=None, random_state=seeds[i])\n",
    "    tree.fit(X_train.iloc[sample],  y_train.iloc[sample])\n",
    "    y_pred.iloc[:,i] = tree.predict(X_test)\n",
    "\n",
    "# Desempeño de cada árbol\n",
    "for i in range(n_estimators):\n",
    "    print('Árbol ', i, 'tiene un accuracy: ', accuracy_score(y_pred.iloc[:,i], y_test))\n",
    "      \n",
    "# Votación mayoritaria\n",
    "y_pred_ = (y_pred.sum(axis=1) >= (n_estimators / 2)).astype(np.int)\n",
    "\n",
    "# Desempeño al hacer votación mayoritaria\n",
    "print('--------------------------------------')\n",
    "accuracyBM = accuracy_score(y_test, y_pred_)\n",
    "print('Accuracy = '+str(accuracyBM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Resultados\n",
    "\n",
    "> Para construir el baggind manueal se consideraron los siguientes puntos: \n",
    "> * Primero se obtuvieron 10 muestras \"Bootstrap\" y para cada una de ellas, se entreno en un árbol de decisión independiente y se hicieron las predicciones correspondientes. \n",
    "> * Al final se combinaron las predicciones de los diferentes árboles, y se seleccionó por votación mayoritaria el resultado debido a que tenemos un problema de clasificación. \n",
    "> * Cada muestra de \"Bootstrap\" se generó del mismo tamaño que el conjunto de entrenamiento original. \n",
    "\n",
    "> Bagging aumenta la precisión predictiva al reducir la varianza de manera similar cómo lo hace la validación cruzada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3 - Bagging con librería\n",
    "\n",
    "En la celda 3, con la librería sklearn, entrenen un modelo bagging con 10 árboles de clasificación y el parámetro `max_features` igual a `log(n_features)`. Presenten el acurracy del modelo en el set de test y comenten sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 0]\n",
      "Accuracy: 0.8429561200923787\n"
     ]
    }
   ],
   "source": [
    "# Celda 3\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "n_estimators=10\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator= DecisionTreeClassifier(max_depth=None,max_features=\"log2\"),\n",
    "                            n_estimators=n_estimators,bootstrap=True, oob_score=True, random_state=1)\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred= bagging.predict(X_test)\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Resultados\n",
    "> Se puede observar que la métrica de Accuracy nos arroja un mejor/peor resultado que el árbol de decisión, pues mientras el primero nos arroja una métrica de 0.887 , el bagging nos arroja 0.842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Random forest con librería\n",
    "\n",
    "En la celda 4, usando la librería sklearn entrenen un modelo de Randon Forest para clasificación y presenten el acurracy del modelo en el set de test y comenten sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP) = 1762\tTrue Negatives (TN) = 1163\tNúmero total de muestras (n) = 3464\n",
      "Accuracy = 0.8443995381062356\n"
     ]
    }
   ],
   "source": [
    "# Celda 4\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test,y_pred).ravel()\n",
    "print('True Positives (TP) = ' + str(tp) + '\\tTrue Negatives (TN) = ' + str(tn) + '\\tNúmero total de muestras (n) = ' + str(X_test.shape[0]))\n",
    "\n",
    "accuracyRF = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy = '+str(accuracyRF))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Resultados\n",
    "\n",
    "> El modelo Random Forest se entrenó con los parametros por default, que en este caso son: \n",
    "> * n_estimators (número de árboles) = 100\n",
    "> * criterion (función para estimar las divisiones) = gini\n",
    "> * max_depth (profundidad máxima del ábol) = None, es decir, los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min_samples_split.\n",
    "> * min_samples_split (número mínimo de muestras requeridas para dividir un nodo interno) = 2 \n",
    "> * max_features (número de variables que se deben considerar en cada árbol) = auto, es decir, se incluyen todas.\n",
    "\n",
    "> Al realizar el calculo del accuray con la muestra de Test se obtuvo un resultado de 0.84, es decir, el modelo acierta el 84% de las veces. Es importante recordar que el accuracy es una métrica para clasificación y esta es la razón por la cuál podemos utilizarla en este ejercicio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Calibración de parámetros Random forest\n",
    "\n",
    "En la celda 5, calibren los parámetros max_depth, max_features y n_estimators del modelo de Randon Forest para clasificación. Presenten el acurracy del modelo en el set de test, comenten sus resultados y análicen cómo cada parámetro afecta el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8911662817551963\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Celda 5\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Creación de lista de valores para iterar sobre diferentes valores de n_estimators\n",
    "estimator_range = range(10, 310, 10)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de n_estimators\n",
    "accuracy_scores = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de n_estimators\n",
    "for estimator in estimator_range:\n",
    "    clf = RandomForestClassifier(n_estimators=estimator, random_state=1, n_jobs=-1)\n",
    "    accuracy_scores.append(cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy').mean())\n",
    "    \n",
    "# Gráfica del desempeño del modelo vs la cantidad de n_estimators\n",
    "plt.plot(estimator_range, accuracy_scores)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Creación de lista de valores para iterar sobre diferentes valores de max_features\n",
    "feature_cols = X_train.columns\n",
    "feature_cols\n",
    "feature_range = range(1, len(feature_cols)+1)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "accuracy_scores1 = []\n",
    "\n",
    "# Uso de un 10-fold cross-validation para cada valor de max_features\n",
    "for feature in feature_range:\n",
    "    clf = RandomForestClassifier(n_estimators=10, max_features=feature, random_state=1, n_jobs=-1)\n",
    "    accuracy_scores1.append(cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy').mean())\n",
    "    \n",
    "# Gráfica del desempeño del modelo vs la cantidad de max_features\n",
    "plt.plot(feature_range, accuracy_scores1)\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Creación de lista de valores para iterar sobre diferentes valores de max_depth\n",
    "estimator_range1 = range(3, 18, 2)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_depth\n",
    "accuracy_scores2 = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de max_depth\n",
    "for max_depth in estimator_range1:\n",
    "    clf = RandomForestClassifier(n_estimators=10, max_depth=max_depth, max_features=8, random_state=1, n_jobs=-1)\n",
    "    accuracy_scores2.append(cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy').mean())\n",
    "    \n",
    "    \n",
    "# Gráfica del desempeño del modelo vs la cantidad de max_depth\n",
    "plt.plot(estimator_range1, accuracy_scores2)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Definición del modelo con los parámetros max_features=8, max_depth=5 y n_estimators=10 \n",
    "clf1 = RandomForestClassifier(n_estimators=10, max_depth=5, max_features=8, random_state=1, n_jobs=-1)\n",
    "clf1.fit(X_test, y_test)\n",
    "y_predRFC= clf1.predict(X_test)\n",
    "accuracyRFC = accuracy_score(y_test, y_predRFC)\n",
    "print('Accuracy = '+str(accuracyRFC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6 - XGBoost con librería\n",
    "\n",
    "En la celda 6 implementen un modelo XGBoost de clasificación con la librería sklearn, presenten el acurracy del modelo en el set de test y comenten sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\wendy\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\wendy\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\wendy\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "[18:30:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score: 0.9047160731472569\n",
      "Accuracy: 0.8856812933025404\n"
     ]
    }
   ],
   "source": [
    "# Celda 6\n",
    "# Importación y definición de modelo XGBClassifier\n",
    "\n",
    "!pip install xgboost\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier()\n",
    "\n",
    "# Entrenamiento (fit) y desempeño del modelo XGBClassifier\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('F1 score: '+ str(metrics.f1_score(y_pred, y_test.values)))\n",
    "print('Accuracy: '+ str(metrics.accuracy_score(y_pred, y_test.values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Resultados \n",
    "> El valor conseguido para el f1_score fue de 0.9047 con un accuracy de 0.8856, es decir un 88,56% de los datos fueron clasificados correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 7 - Calibración de parámetros XGBoost\n",
    "\n",
    "En la celda 7 calibren los parámetros learning rate, gamma y colsample_bytree del modelo XGBoost para clasificación. Presenten el acurracy del modelo en el set de test, comenten sus resultados y análicen cómo cada parámetro afecta el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor parametro gamma con el debido resultado\n",
      "{'gamma': 0.0}\n",
      "0.8742719040440956\n",
      "Mejor parametro learning_rate con el debido resultado\n",
      "{'learning_rate': 0.01}\n",
      "0.8796768039501739\n",
      "Mejor parametro colsample_bytree con el debido resultado\n",
      "{'colsample_bytree': 0.7}\n",
      "0.8812413243677973\n",
      "Resultado con los mejores parametros:\n",
      "F1 score: 0.9073588949749941\n",
      "Accuracy: 0.8877020785219399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 7\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Calibrar el parametro gamma\n",
    "\n",
    "# Por medio de GridSearchCV se buscara el valor para el parametro gamma que nos arroje mejores resultados dentro de los valores\n",
    "# tipicos que se manejan. GridSearch nos ayuda a iterar con los valores especificados para obtener la\n",
    "# mayor precisión\n",
    "\n",
    "# Se define el rango de los valores tipicos que utiliza el parametro gamma\n",
    "param_gamma = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "# Por medio de GridSearchCV se recorreran todas las combinaciones posibles establecidas anteriormente para poder\n",
    "# encontrar la que mayor precisión nos ofrece\n",
    "gsearch_1 = GridSearchCV(estimator = XGBClassifier(eval_metric='error'), \n",
    " param_grid = param_gamma, cv=5)\n",
    "\n",
    "# Se procede a entrenar el modelo\n",
    "gsearch_1.fit(X_train, y_train)\n",
    "\n",
    "print('Mejor parametro gamma con el debido resultado')\n",
    "best_gamma=gsearch_1.best_params_['gamma']\n",
    "print(gsearch_1.best_params_), print(gsearch_1.best_score_)\n",
    "\n",
    "\n",
    "# Calibrar el parametro learning_rate\n",
    "\n",
    "# Se define el rango de los valores tipicos que utiliza el parametro learning_rate\n",
    "param_learning_rate = {\n",
    " 'learning_rate' : [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Por medio de GridSearchCV se recorreran todas las combinaciones posibles establecidas anteriormente para poder\n",
    "# encontrar la que mayor precisión nos ofrece\n",
    "gsearch_2 = GridSearchCV(estimator = XGBClassifier(gamma=best_gamma, eval_metric='error'), \n",
    " param_grid = param_learning_rate, cv=5)\n",
    "\n",
    "# Se procede a entrenar el modelo\n",
    "gsearch_2.fit(X_train, y_train)\n",
    "\n",
    "print('Mejor parametro learning_rate con el debido resultado')\n",
    "best_learning_rate=gsearch_2.best_params_['learning_rate']\n",
    "print(gsearch_2.best_params_), print(gsearch_2.best_score_)\n",
    "\n",
    "\n",
    "# Calibrar el parametro colsample_bytree\n",
    "\n",
    "# Se define el rango de los valores tipicos que utiliza el parametro best_learning_rate\n",
    "param_colsample_bytree = {\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "# Por medio de GridSearchCV se recorreran todas las combinaciones posibles establecidas anteriormente para poder\n",
    "# encontrar la que mayor precisión nos ofrece\n",
    "gsearch_3 = GridSearchCV(estimator = XGBClassifier(gamma=best_gamma, learning_rate=best_learning_rate, eval_metric='error'), \n",
    " param_grid = param_colsample_bytree, cv=5)\n",
    "\n",
    "# Se procede a entrenar el modelo\n",
    "gsearch_3.fit(X_train, y_train)\n",
    "\n",
    "print('Mejor parametro colsample_bytree con el debido resultado')\n",
    "best_colsample_bytree=gsearch_3.best_params_['colsample_bytree']\n",
    "print(gsearch_3.best_params_), print(gsearch_3.best_score_)\n",
    "\n",
    "\n",
    "# Entrenamiento (fit) y desempeño del modelo XGBClassifier con los parametros obtenidos\n",
    "clf = XGBClassifier(gamma=best_gamma, learning_rate=best_learning_rate,colsample_bytree=best_colsample_bytree,eval_metric='error')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Resultado con los mejores parametros:')\n",
    "print('F1 score: '+str(metrics.f1_score(y_pred, y_test.values))), print('Accuracy: '+str(metrics.accuracy_score(y_pred, y_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Resultados\n",
    "> Mientras se iban calibrando los parametros se pudo observar como mejoraban los resultados, al utilizar los 3 parametros finales se obtuvo un accuracy de 0,8877 con el cual se mejoraron los resultados obtenidos en el punto 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 8 - Comparación y análisis de resultados\n",
    "En la celda 8 comparen los resultados obtenidos de los diferentes modelos (random forest y XGBoost) y comenten las ventajas del mejor modelo y las desventajas del modelo con el menor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
